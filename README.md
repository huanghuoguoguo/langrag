<div align="center">
  <a href="README.md">English</a> | <a href="README_zh.md">ä¸­æ–‡</a>
</div>

<p align="center">
  <img src="docs/logo.svg" alt="LangRAG Logo" width="300"/>
</p>

<h1 align="center">LangRAG</h1>

<p align="center">
  <strong>A Modular, Production-Ready RAG Kernel for Building Intelligent Knowledge Systems</strong>
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#documentation">Documentation</a> â€¢
  <a href="#roadmap">Roadmap</a> â€¢
  <a href="#comparison">Comparison</a>
</p>

---

## What is LangRAG?

**LangRAG** is a **"Small and Beautiful" RAG Kernel**. It is designed to be the lightweight, robust engine at the heart of your intelligent knowledge systems.

LangRAG strikes a unique balance: it implements **industry-standard best practices** (like Parent-Child Indexing, Hybrid Search, and LLM Judges) while maintaining a **minimal footprint** and a **flat, transparent code structure**.

We believe in an **Out-of-the-Box** philosophy that doesn't sacrifice control. You get production-ready primitivesâ€”with built-in Telemetry, Caching, and Evaluationâ€”without the weight and "magic" of monolithic frameworks.

> ğŸ¯ **Why LangRAG?**
> *   **Opinionated & Ready**: Best-practices like RRF and Recursive Chunking are the default, not a config hell.
> *   **Transparent Kernel**: ~3k LOC core with no deep abstraction layers. You can read, understand, and mod the code in minutes.
> *   **Application Driven**: LangRAG is a library you use, not a framework that uses you.

The `web/` directory contains a **demo application** showcasing how a sophisticated, "industry-grade" RAG flow can be built with minimal glue code.

---

## Features

### âœ… Implemented (v0.2)

| Category | Feature | Description |
|----------|---------|-------------|
| **Indexing** | Multi-Format Parsing | PDF, DOCX, Markdown, HTML, TXT |
| | Smart Chunking | Recursive Character Splitter with overlap |
| | Parent-Child Indexing | Hierarchical retrieval for long documents |
| | QA Indexing | Question-Answer pair extraction for precise matching |
| | Batch Processing | Efficient large-scale document indexing with progress tracking |
| **Storage** | Vector Stores | DuckDB (persistent, hybrid search), ChromaDB, SeekDB (hybrid) |
| | KV Store | SQLite-based persistent key-value storage |
| | Web Search | Real-time web integration (Bing, Google, DuckDuckGo) |
| **Retrieval** | Hybrid Search | Vector + BM25 Full-text with RRF fusion (DuckDB, SeekDB) |
| | Agentic Router | LLM-powered knowledge base selection |
| | Query Rewriter | Semantic query optimization |
| | Reranker | Cohere, Qwen, NoOp providers |
| | Semantic Cache | Similarity-based query caching with TTL and LRU eviction |
| **Evaluation** | LLM Judge | Faithfulness, Answer Relevancy, Context Relevancy metrics |
| | Batch Evaluation | Evaluate multiple samples with progress callbacks |
| | Evaluation Report | Aggregated statistics and per-sample results |
| **Observability** | OpenTelemetry | Distributed tracing for retrieval and indexing pipelines |
| **Generation** | Streaming | Server-Sent Events for real-time responses |
| | LLM Abstraction | OpenAI-compatible interface with injection |
| | Multi-Stage LLM | Stage-based model configuration (chat, router, rewriter, reranker) |
| **Testing** | Full Suite | Unit, Integration tests (500+ tests) |

### ğŸ”§ Architecture Highlights

- **Dependency Injection**: LLM, Embedder, and VectorStore are injected, not managed internally.
- **Multi-Stage LLM**: Configure different models for different tasks (chat, router, rewriter, reranker).
- **Factory Pattern**: Easily register and create custom components.
- **Async-First**: Core APIs support async/await for high concurrency.
- **Type-Safe**: Pydantic models for all configurations and entities.
- **Observable**: Built-in OpenTelemetry tracing support.

---

## Architecture

```
src/langrag/
â”œâ”€â”€ config/            # Configuration management (Pydantic)
â”œâ”€â”€ core/              # Callbacks and event system
â”œâ”€â”€ datasource/        # Storage abstractions
â”‚   â”œâ”€â”€ kv/            # Key-Value stores (InMemory, SQLite)
â”‚   â””â”€â”€ vdb/           # Vector databases (Chroma, DuckDB, SeekDB, Web)
â”œâ”€â”€ entities/          # Domain models (Document, Dataset, SearchResult)
â”œâ”€â”€ index_processor/   # Indexing pipeline
â”‚   â”œâ”€â”€ extractor/     # Document parsers (PDF, DOCX, MD, HTML, TXT)
â”‚   â”œâ”€â”€ splitter/      # Text chunkers (Recursive, FixedSize)
â”‚   â”œâ”€â”€ processor/     # Index strategies (Paragraph, ParentChild, QA)
â”‚   â””â”€â”€ cleaner/       # Text normalization
â”œâ”€â”€ llm/               # LLM abstractions
â”‚   â”œâ”€â”€ embedder/      # Embedding providers
â”‚   â”œâ”€â”€ providers/     # LLM providers (OpenAI-compatible, local)
â”‚   â””â”€â”€ stages.py      # Multi-stage LLM configuration (chat, router, rewriter, reranker)
â”œâ”€â”€ retrieval/         # Retrieval pipeline
â”‚   â”œâ”€â”€ router/        # Knowledge base routing
â”‚   â”œâ”€â”€ rewriter/      # Query rewriting
â”‚   â”œâ”€â”€ rerank/        # Result reranking
â”‚   â”œâ”€â”€ compressor/    # Context compression
â”‚   â””â”€â”€ workflow.py    # Orchestration
â”œâ”€â”€ cache/             # Semantic caching layer
â”œâ”€â”€ batch/             # Batch processing for large-scale indexing
â”œâ”€â”€ evaluation/        # LLM Judge evaluation framework
â”‚   â””â”€â”€ metrics/       # Faithfulness, Answer/Context Relevancy
â”œâ”€â”€ observability/     # OpenTelemetry tracing integration
â””â”€â”€ utils/             # Utilities (RRF, similarity, async helpers)
```

---

## Quick Start

### Installation

```bash
git clone https://github.com/huanghuoguoguo/langrag.git
cd langrag

# Install with uv (recommended)
uv sync --dev
```

### Option 1: Run the Web Demo

```bash
./web/start.sh
# Or: uv run python -m web.app
```

Visit: [http://localhost:8000](http://localhost:8000)

### Option 2: Use as a Library

```python
from langrag import (
    Dataset,
    SimpleTextParser,
    RecursiveCharacterChunker,
    ParentChildIndexProcessor
)
from langrag.datasource.vdb.duckdb import DuckDBVector
from langrag.datasource.kv.sqlite import SQLiteKV

# 1. Parse documents
parser = SimpleTextParser()
docs = parser.parse("knowledge_base.txt")

# 2. Create dataset and stores
dataset = Dataset(name="my_kb", collection_name="my_collection")
vector_store = DuckDBVector(dataset, database_path="./vectors.duckdb")
kv_store = SQLiteKV(db_path="./parents.sqlite")

# 3. Index with Parent-Child strategy
processor = ParentChildIndexProcessor(
    vector_store=vector_store,
    kv_store=kv_store,
    embedder=my_embedder,  # Inject your embedder
    parent_splitter=...,
    child_splitter=...
)
processor.process(dataset, docs)

# 4. Search
results = vector_store.search("your query", query_vector=[...], top_k=5)
```

---

## Documentation

LangRAG uses MkDocs with Material theme for comprehensive documentation.

### View Documentation

```bash
# Install documentation dependencies
uv sync --extra docs

# Serve documentation locally
uv run mkdocs serve

# Build static documentation
uv run mkdocs build
```

Visit: [http://localhost:8000](http://localhost:8000) for local docs.

### Documentation Structure

- **Getting Started**: Installation, Quick Start
- **User Guide**: Core Concepts, Document Processing, Retrieval Workflow, Evaluation
- **API Reference**: Complete API documentation for all modules

---

## Roadmap

### âœ… v0.2 (Completed)

- [x] **DuckDB FTS**: Full-text search with BM25 and RRF hybrid fusion
- [x] **Semantic Cache**: Similarity-based caching with TTL and LRU eviction
- [x] **Batch Processing**: Large-scale document indexing with progress tracking
- [x] **LLM Judge**: Evaluation framework (Faithfulness, Answer/Context Relevancy)
- [x] **OpenTelemetry**: Distributed tracing integration
- [x] **API Documentation**: MkDocs-based comprehensive documentation

### ğŸš€ v0.3 (In Progress)
- [x] **Agents**: Tool-use and multi-step reasoning framework
- [x] **RAPTOR**: Recursive Abstractive Processing for Tree-Organized Retrieval
- [ ] **Graph RAG**: Knowledge graph integration
- [ ] **Adaptive Retrieval**: Dynamic strategy selection based on query type
- [ ] **Evaluation Benchmark**: Built-in eval datasets (BEIR, MTEB)

### Future
- [ ] **Multi-Modal**: Image and audio document support
- [ ] **Cloud Connectors**: S3, GCS, Azure Blob for document ingestion

---

## Comparison with Other RAG Frameworks

| Feature | LangRAG | LangChain | LlamaIndex | PowerRAG |
|---------|---------|-----------|------------|----------|
| **Focus** | RAG Kernel | General LLM Framework | Data Framework | Production Platform |
| **Philosophy** | Inject, Don't Manage | All-in-one | Index-centric | Service-Oriented (DB-centric) |
| **Storage** | Flexible (DuckDB/SeekDB) | Agnostic | Agnostic | OceanBase (SQL+Vector) |
| **Agentic Router** | âœ… LLM-powered | âœ… Chains | âœ… Router | âœ… Conversational |
| **Parent-Child Indexing** | âœ… Built-in | âœ… Supported | âœ… Supported | âœ… Supported |
| **RAA/RAPTOR** | âœ… Built-in | âš ï¸ Manual | âœ… Supported | âš ï¸ Manual |
| **Hybrid Search** | âœ… DuckDB, SeekDB | âœ… Ensemble | âœ… External | âœ… OceanBase |
| **Semantic Cache** | âœ… Built-in | âŒ External | âŒ External | âŒ External |
| **LLM Judge Evaluation** | âœ… Built-in | âš ï¸ Integration | âœ… Built-in | âœ… Integration (Langfuse) |
| **OpenTelemetry** | âœ… Native | âš ï¸ Partial | âš ï¸ Partial | âš ï¸ Integration |
| **Web Search Integration** | âœ… Multi-provider | âœ… Tools | âœ… Tools | âœ… Tools |
| **Lightweight** | âœ… ~3k LOC core | âŒ Large | âŒ Large | âŒ Heavy (Docker Compose) |
| **Type Safety** | âœ… Pydantic | âš ï¸ Partial | âœ… Pydantic | âœ… Pydantic |

### Why Choose LangRAG?

1. **Kernel, Not Framework**: LangRAG gives you RAG primitives without imposing an application structure.
2. **Injection-First**: Your app owns the LLM, Embedder, and storage. LangRAG just orchestrates.
3. **Advanced Indexing**: Built-in Parent-Child and QA indexing strategies out of the box.
4. **Built-in Evaluation**: LLM Judge framework for retrieval quality assessment.
5. **Production-Ready**: Semantic caching, batch processing, and OpenTelemetry tracing.
6. **Comprehensive Testing**: 500+ tests with thorough edge case coverage.
7. **Minimal Dependencies**: Core library has minimal external dependencies.

---

## Development

```bash
# Run core tests (LangRAG library)
uv run pytest tests/

# Run web demo tests
uv run pytest web/tests/ -m "not local_llm"

# Run integration tests
uv run pytest tests/integration/

# Run with coverage
./run_tests.sh

# Lint and format
uv run ruff check src/ tests/
uv run ruff format src/ tests/
```

### Test Structure

```
tests/              # LangRAG core tests
â”œâ”€â”€ unit/           # Unit tests (464 tests)
â””â”€â”€ integration/    # Integration tests (DuckDB, SeekDB verification)

web/tests/          # Web Demo tests
â”œâ”€â”€ unit/           # Unit tests for web components
â””â”€â”€ test_api.py     # API integration tests
```

### Optional Dependencies

```bash
# Document parsers (PDF, DOCX, etc.)
pip install langrag[parsers]

# Reranker support
pip install langrag[reranker]

# OpenTelemetry observability
pip install langrag[observability]

# Documentation generation
pip install langrag[docs]

# All features
pip install langrag[all]
```

---

## License

MIT License

---

<p align="center">
  <sub>Built with â¤ï¸ for the RAG community</sub>
</p>
